# semantic-book-recommender
A content-based recommendation engine using Multilingual Sentence Transformers (BERT) to map English and Arabic books into a shared semantic vector space for personalized discovery.
This is a great clarification. The fact that you have two distinct modes (Single Book Search vs. Full Profile Upload) is a major feature. It shows the tool is flexibleâ€”useful for a quick search and deep personalization.Here is the final, polished README.md. I have updated the "Usage" and "Features" sections to explicitly highlight these two workflows (Book-to-Book vs. Profile-to-Book).Copy this directly into your GitHub.ðŸ“š Semantic Book Recommender (Multilingual)This project is an end-to-end Natural Language Processing (NLP) solution designed to provide personalized book recommendations across languages. By leveraging Sentence Transformers (BERT), the model bridges the gap between English and Arabic literature, recommending books based on semantic meaning rather than just keyword matching.The system ingests data from Goodreads (User History) and Hugging Face (Arabic E-Book Corpus) to build a unified vector space for literary discovery.1. Problem DescriptionThe goal of this project is to solve two specific problems in the recommendation space:The Vocabulary Gap: Standard algorithms (like TF-IDF) fail to link concepts that use different words (e.g., "Space Travel" vs. "Interstellar Journey").The Language Barrier: A user who reads English Sci-Fi might love Arabic Sci-Fi, but traditional models keep these datasets separate.This solution implements a Content-Based Filtering engine that treats books as "dense vectors." By analyzing metadata (Title, Author, Description, Genres), the model calculates a "User Taste Centroid"â€”a mathematical representation of a user's unique preferenceâ€”to predict their next favorite book with high precision.2. Data Engineering & StrategyMy analysis focused on merging disparate datasets to create a "Global Library." I utilized the "Soup Strategy" to engineer features that capture the full context of a book.1. Data Sources:Goodreads Data: Accepts a standard goodreads_export.csv file containing a user's reading history, ratings, and shelves.Arabic Corpus: Integrated the mohres/The_Arabic_E-Book_Corpus from Hugging Face to expand the library with thousands of Arabic titles.2. Feature Engineering (The "Soup"):To ensure the model understands context, I concatenated all relevant metadata into a single text string for each book.FeatureSourcePurposeTitleGoodreads/HFCore identity of the item.AuthorGoodreads/HFCaptures writing style preferences.DescriptionGoodreads/HFProvides the plot and thematic context.GenresGoodreads/HFHigh-level categorization (e.g., "Sci-Fi", "Philosophy").3. Multilingual Unification:By merging these datasets into a single DataFrame (master_df), I enabled the model to perform Cross-Lingual Searchâ€”finding Arabic matches for English queries without needing translation.3. Model Architecture & SelectionI initially considered TF-IDF (Term Frequency-Inverse Document Frequency) but rejected it in favor of Sentence Transformers.Why Transformers?TF-IDF (Sparse): Creates a massive, mostly empty matrix. It cannot understand that "Sad" and "Melancholy" are related.Transformers (Dense): Converts text into a compact list of numbers (embeddings). It understands that "King - Man + Woman = Queen."The Model: paraphrase-multilingual-MiniLM-L12-v2Architecture: BERT-based Transformer.Dimensions: 384-dimensional dense vectors.Language Support: 50+ languages (enabling the English-Arabic bridge).4. Usage & WorkflowsThe system offers two distinct modes of operation depending on the user's needs:Mode 1: Single Book Search (Book-to-Book)Function: get_recommendations(query, ...)Input: A specific book title (e.g., "The Saga of Arthur Bandini").Process: The model converts the query into a vector and finds the nearest neighbors in the shared vector space.Output: Returns top 5 semantically similar books (English or Arabic).Mode 2: Profile Personalization (Profile-to-Book)Function: get_profile_recommendations(my_df, ...)Input: Upload your personal goodreads_export.csv.Process (The "Taste Centroid"):Filter: Selects books the user rated 4 stars or higher.Vectorize: Converts these specific books into embeddings.Average: Calculates the Mean Vector (Centroid) of these books to represent the user's "Taste Profile."Search: Finds unread books closest to this centroid.5. ReproducibilityThis project is fully reproducible using Python.Prerequisites:You will need the following libraries:Bashpip install pandas numpy scikit-learn sentence-transformers datasets
To Run the Analysis:Clone the Repository:Bashgit clone https://github.com/ali-elberier/semantic-book-recommender.git
Load the Notebook:Open goodreads_reccomndation.ipynb.Execute:Run all cells. The notebook will automatically download the Arabic corpus from Hugging Face and generate embeddings.6. Example ResultsBelow is an actual output from the model demonstrating its ability to find thematically similar books.Input Book: "The Saga of Arthur Bandini" (Themes: Struggle, Poverty, Writing)Model Recommendations (Top 5):ScoreBook TitleInsight0.545Cyclops / Alcestis / MedeaThematic match on tragedy/struggle.0.545The Charterhouse of ParmaClassic literature with similar depth.0.508Ø§Ù„Ø«Ø§Ø¨Øª (The Constant)Cross-Lingual Discovery: An Arabic title identified as semantically similar.0.505Jacques and His MasterPhilosophical fiction match.User Profile Prediction:Input: User history includes "The Alchemist" and "The Prophet".Prediction: "Siddhartha" (0.78 Similarity).Result: Accurate. The model correctly identified "Philosophical Self-Discovery" as the user's core interest.
